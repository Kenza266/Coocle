{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from Index import Index \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(f):\n",
    "    content = ''\n",
    "    for line in f:\n",
    "        content += line\n",
    "    return content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cisi\\\\CISI.ALL', 'r') as f:\n",
    "    content = read(f)\n",
    "titles = [x.groups()[0] for x in re.finditer(r\"\\.T[^\\.\\w]*?\\n((.|\\n)*?)\\n\\.A\", content)]\n",
    "abstracts = [x.groups()[0] for x in re.finditer(r\"\\.W.*?\\n((.|\\n)*?)\\n\\.X\", content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cisi\\\\CISI.QRY', 'r') as f:\n",
    "    content = read(f)\n",
    "queries = [x.groups()[0] for x in re.finditer(r\"\\.W[^\\.\\w]*?\\n((.|\\n)*?)\\n\\.\", content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 1460, 112)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles), len(abstracts), len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [t + ' ' + a for t, a in zip(titles, abstracts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:03<00:00, 366.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing empty words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 3162.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 65264.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:04, 319.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 3241.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:04, 320.82it/s]\n"
     ]
    }
   ],
   "source": [
    "index = Index(docs) \n",
    "index.process() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index and Inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.get_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.get_inverted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(index.index, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverted.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(index.inverted, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for k in list(index.index.keys()):\n",
    "    for v in index.index[k]:\n",
    "        data.append([k] + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.csv\", \"wt\", newline='') as fp:\n",
    "    writer = csv.writer(fp, delimiter=\",\")\n",
    "    writer.writerow(['Document', 'Token', 'Frequency', 'Weight']) \n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(token):\n",
    "    t = PorterStemmer().stem(token)\n",
    "    return WordNetLemmatizer().lemmatize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(docs, regex='(?:[A-Za-z]\\.)+|\\d+(?:\\.\\d+)?%?|\\w+(?:\\-\\w+)*'):\n",
    "    regex = nltk.RegexpTokenizer(regex) \n",
    "    tokens_lists = [regex.tokenize(txt) for txt in docs]\n",
    "    tokens_lists = [[filter(t) for t in tokens_list] for tokens_list in tokens_lists] \n",
    "    empty_words = nltk.corpus.stopwords.words('english')\n",
    "    tokens_lists = [[token.lower() for token in tokens if token not in empty_words] for tokens in tokens_lists]\n",
    "    return tokens_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = tokenize(queries)\n",
    "queries = [list(np.unique(q)) for q in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {i: q for i, q in enumerate(queries)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('queries.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(queries, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('cisi\\\\CISI.REL', 'r')\n",
    "truth = [x.split()[:2] for x in f.readlines()]\n",
    "with open(\"ground_truth.csv\", \"wt\", newline='') as fp:\n",
    "    writer = csv.writer(fp, delimiter=\",\")\n",
    "    writer.writerow(['Query', 'Relevent document']) \n",
    "    writer.writerows(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Index():\n",
    "    def __init__(self, docs, preprocessed=False):\n",
    "        if preprocessed:\n",
    "            index, inverted, queries, ground_truth = docs\n",
    "            self.index = index\n",
    "            self.inverted = inverted \n",
    "            self.queries = queries\n",
    "            self.ground_truth = ground_truth\n",
    "        else:\n",
    "            self.docs = docs\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.port = PorterStemmer()        \n",
    "\n",
    "    def filter(self, token):\n",
    "        t = self.port.stem(token)\n",
    "        return self.wnl.lemmatize(t)\n",
    "\n",
    "    def tokenize(self, regex='(?:[A-Za-z]\\.)+|\\d+(?:\\.\\d+)?%?|\\w+(?:\\-\\w+)*'):\n",
    "        regex = nltk.RegexpTokenizer(regex) \n",
    "        self.tokens_lists = [regex.tokenize(txt) for txt in self.docs]\n",
    "        self.tokens_lists = [[self.filter(t) for t in tokens_list] for tokens_list in tqdm(self.tokens_lists)] \n",
    "        empty_words = nltk.corpus.stopwords.words('english')\n",
    "        print('Removing empty words...')\n",
    "        self.tokens_lists = [[token.lower() for token in tokens if token not in empty_words] for tokens in tqdm(self.tokens_lists)]\n",
    "    \n",
    "    def get_freq(self):\n",
    "        self.frequencies = []\n",
    "        for tokens in tqdm(self.tokens_lists):\n",
    "            dict = {}\n",
    "            for token in tokens:\n",
    "                dict[token] = (dict[token] if token in dict.keys() else 0) + 1\n",
    "            self.frequencies.append(dict) \n",
    "\n",
    "    def get_weights(self):\n",
    "        max = [np.max(list(d.values())) for d in self.frequencies]\n",
    "        self.weights = []\n",
    "        for i in tqdm(range(len(max))):\n",
    "            d = {}\n",
    "            for k, v in self.frequencies[i].items():\n",
    "                d[k] = round((v/max[i]) * np.log10(len(max)/len(self.get_docs(k, preprocessed=True))+1), 2)\n",
    "            self.weights.append(d)\n",
    "        \n",
    "    def combine(self, origin):\n",
    "        out = {}\n",
    "        sets = set()\n",
    "        for o in origin:\n",
    "            sets = sets | set(o)\n",
    "        frequencies = [{k: origin[i].get(k) if origin[i].get(k) else  0 for k in sets} for i in range(len(origin))]\n",
    "        for freq, d in tqdm(zip(frequencies, range(len(frequencies)))):\n",
    "            for k, v in freq.items():\n",
    "                out[(k, d)] = v\n",
    "        return out\n",
    "\n",
    "    def process(self):\n",
    "        print('Tokenizing...')\n",
    "        self.tokenize()\n",
    "        print('Getting frequencies...')\n",
    "        self.get_freq()\n",
    "        print('Combining...')\n",
    "        self.all_frequencies = self.combine(self.frequencies) \n",
    "        self.get_inverted_f()\n",
    "        print('Getting weights...')\n",
    "        self.get_weights()\n",
    "        print('Combining...')\n",
    "        self.all_weights = self.combine(self.weights) \n",
    "        self.get_index()\n",
    "        self.get_inverted()\n",
    "        \n",
    "    def get_index(self):\n",
    "        index = {}\n",
    "        for doc, (w, f) in enumerate(zip(self.weights, self.frequencies)):\n",
    "            d = []\n",
    "            for token in list(w.keys()):\n",
    "                d.append([token, f[token], w[token]])\n",
    "            index[doc] = d\n",
    "        self.index = index\n",
    "\n",
    "    def get_inverted_f(self):\n",
    "        inverted = {}\n",
    "        for doc, f in enumerate(self.frequencies):\n",
    "            for token in list(f.keys()):\n",
    "                if token in inverted.keys():\n",
    "                    inverted[token].append([doc, f[token]]) \n",
    "                else:\n",
    "                    inverted[token] = [[doc, f[token]]]\n",
    "        self.inverted = inverted\n",
    "\n",
    "    def get_inverted(self):\n",
    "        inverted = {}\n",
    "        for doc, (w, f) in enumerate(zip(self.weights, self.frequencies)):\n",
    "            for token in list(w.keys()):\n",
    "                if token in inverted.keys():\n",
    "                    inverted[token].append([doc, f[token], w[token]]) \n",
    "                else:\n",
    "                    inverted[token] = [[doc, f[token], w[token]]]\n",
    "        self.inverted = inverted\n",
    "        \n",
    "    def get_docs_per_token(self, token):\n",
    "        f = {d: v for (t, d), v in self.all_frequencies.items() if token == t} \n",
    "        f = [i for i in list(f.values()) if i != 0]\n",
    "        return f \n",
    "    \n",
    "    def get_docs(self, token, preprocessed=False):\n",
    "        if not preprocessed:\n",
    "            token = self.filter(token.lower())\n",
    "        return self.inverted[token]\n",
    "\n",
    "    def get_docs_query(self, query):\n",
    "        tokens = [token for token in query.split()]\n",
    "        all = {}\n",
    "        details = {}\n",
    "        for token in tokens:\n",
    "            docs = self.get_docs(token)\n",
    "            for d in docs:\n",
    "                if d[0] not in all.keys():\n",
    "                    all[d[0]] = [[d[1]], [d[2]]]\n",
    "                else:\n",
    "                    all[d[0]] = [[all[d[0]][0][0] + d[1]], [round(all[d[0]][1][0] + d[2], 2)]]\n",
    "            details[token] = {d[0]: [d[1], d[2]] for d in docs}\n",
    "        return details, all\n",
    "\n",
    "    def scalar_prod(self, doc, query):\n",
    "        result = 0\n",
    "        for token in doc:\n",
    "            if token in query:\n",
    "                result += np.sum([l[2] for l in self.get_docs(token, preprocessed=True)])\n",
    "        return result\n",
    "\n",
    "    def cosine_measure(self, doc, query):\n",
    "        w = [self.get_docs(token, preprocessed=True)[0][2] for token in doc]\n",
    "        result = np.sqrt(len(query)) * np.sqrt(np.dot(w, w))\n",
    "        return self.scalar_prod(doc, query) / result\n",
    "\n",
    "    def jaccard_measure(self, doc, query):\n",
    "        w = [self.get_docs(token, preprocessed=True)[0][2] for token in doc]\n",
    "        result = len(query) + np.dot(w, w) - self.scalar_prod(doc, query)\n",
    "        return self.scalar_prod(doc, query) / result\n",
    "\n",
    "    def vector_search(self, max_docs=50, metric='scalar'):\n",
    "        queries = np.unique(self.ground_truth['Query'])\n",
    "        relevent_docs = [list(self.ground_truth[self.ground_truth['Query'] == q]['Relevent document']) for q in queries]\n",
    "        predicted = {}\n",
    "        if metric == 'scalar':\n",
    "            metric = self.scalar_prod\n",
    "        elif metric == 'cosine':\n",
    "            metric = self.cosine_measure\n",
    "        elif metric == 'jaccard':\n",
    "            metric = self.jaccard_measure\n",
    "        #max_docs = max([len(l) for l in relevent_docs])\n",
    "        for q in tqdm(queries):\n",
    "            pred = []\n",
    "            query = self.queries[str(q)]\n",
    "            for doc, tokens in self.index.items():\n",
    "                pred.append([q, doc, metric([t[0] for t in tokens], query)])\n",
    "            pred = sorted(pred, key=lambda x: x[2], reverse=True)\n",
    "            predicted[q] = [p[1] for p in pred[:max_docs]]\n",
    "        return predicted.values(), relevent_docs\n",
    "\n",
    "    def PR(self, pred, relevent):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        for p, r in zip(pred, relevent):\n",
    "            TP = len(set(p) & set(r))\n",
    "            precisions.append(TP/len(p))\n",
    "            recalls.append(TP/len(r))\n",
    "        return np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('queries.json') as f:\n",
    "    queries = json.load(f) \n",
    "ground_truth = pd.read_csv('ground_truth.csv', sep=',')\n",
    "index = Index((index.index, index.inverted, queries, ground_truth), preprocessed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:22<00:00,  3.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0319078947368421, 0.15407759855924844)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, relevent_docs = index.vector_search(max_docs=200, metric='scalar')\n",
    "index.PR(list(predicted), relevent_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:31<00:00,  2.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.030592105263157896, 0.15148522019219993)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, relevent_docs = index.vector_search(max_docs=200, metric='cosine')\n",
    "index.PR(list(predicted), relevent_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [01:04<00:00,  1.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.027631578947368427, 0.15059664050403268)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, relevent_docs = index.vector_search(max_docs=200, metric='jaccard')\n",
    "index.PR(list(predicted), relevent_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5addf786bcd861d1ce5006f23111f8cbb206731e5b61b0a5632ba9e0252558a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
