{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from Index import Index \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(f):\n",
    "    content = ''\n",
    "    for line in f:\n",
    "        content += line\n",
    "    return content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cisi\\\\CISI.ALL', 'r') as f:\n",
    "    content = read(f)\n",
    "titles = [x.groups()[0] for x in re.finditer(r\"\\.T[^\\.\\w]*?\\n((.|\\n)*?)\\n\\.A\", content)]\n",
    "abstracts = [x.groups()[0] for x in re.finditer(r\"\\.W.*?\\n((.|\\n)*?)\\n\\.X\", content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cisi\\\\CISI.QRY', 'r') as f:\n",
    "    content = read(f)\n",
    "queries = [x.groups()[0] for x in re.finditer(r\"\\.W[^\\.\\w]*?\\n((.|\\n)*?)\\n\\.\", content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 1460, 112)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles), len(abstracts), len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [t + ' ' + a for t, a in zip(titles, abstracts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:04<00:00, 342.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing empty words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 2402.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 51889.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:05, 250.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 2140.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:05, 276.00it/s]\n"
     ]
    }
   ],
   "source": [
    "index = Index(docs) \n",
    "index.process() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index and Inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.get_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.get_inverted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(index.index, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverted.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(index.inverted, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for k in list(index.index.keys()):\n",
    "    for v in index.index[k]:\n",
    "        data.append([k] + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.csv\", \"wt\", newline='') as fp:\n",
    "    writer = csv.writer(fp, delimiter=\",\")\n",
    "    writer.writerow(['Document', 'Token', 'Frequency', 'Weight']) \n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(token):\n",
    "    t = PorterStemmer().stem(token)\n",
    "    return WordNetLemmatizer().lemmatize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(docs, regex='(?:[A-Za-z]\\.)+|\\d+(?:\\.\\d+)?%?|\\w+(?:\\-\\w+)*'):\n",
    "    regex = nltk.RegexpTokenizer(regex) \n",
    "    tokens_lists = [regex.tokenize(txt) for txt in docs]\n",
    "    tokens_lists = [[filter(t) for t in tokens_list] for tokens_list in tokens_lists] \n",
    "    empty_words = nltk.corpus.stopwords.words('english')\n",
    "    tokens_lists = [[token.lower() for token in tokens if token not in empty_words] for tokens in tokens_lists]\n",
    "    return tokens_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = tokenize(queries)\n",
    "queries = [list(np.unique(q)) for q in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {i: q for i, q in enumerate(queries)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('queries.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(queries, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('cisi\\\\CISI.REL', 'r')\n",
    "truth = [x.split()[:2] for x in f.readlines()]\n",
    "with open(\"ground_truth.csv\", \"wt\", newline='') as fp:\n",
    "    writer = csv.writer(fp, delimiter=\",\")\n",
    "    writer.writerow(['Query', 'Relevent document']) \n",
    "    writer.writerows(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Index():\n",
    "    def __init__(self, docs, preprocessed=False):\n",
    "        if preprocessed:\n",
    "            index, inverted, queries, ground_truth = docs\n",
    "            self.index = index\n",
    "            self.inverted = inverted \n",
    "            self.queries = queries\n",
    "            self.ground_truth = ground_truth\n",
    "        else:\n",
    "            self.docs = docs\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.port = PorterStemmer()        \n",
    "\n",
    "    def filter(self, token):\n",
    "        t = self.port.stem(token)\n",
    "        return self.wnl.lemmatize(t)\n",
    "\n",
    "    def tokenize(self, regex='(?:[A-Za-z]\\.)+|\\d+(?:\\.\\d+)?%?|\\w+(?:\\-\\w+)*'):\n",
    "        regex = nltk.RegexpTokenizer(regex) \n",
    "        self.tokens_lists = [regex.tokenize(txt) for txt in self.docs]\n",
    "        self.tokens_lists = [[self.filter(t) for t in tokens_list] for tokens_list in tqdm(self.tokens_lists)] \n",
    "        empty_words = nltk.corpus.stopwords.words('english')\n",
    "        print('Removing empty words...')\n",
    "        self.tokens_lists = [[token.lower() for token in tokens if token not in empty_words] for tokens in tqdm(self.tokens_lists)]\n",
    "    \n",
    "    def get_freq(self):\n",
    "        self.frequencies = []\n",
    "        for tokens in tqdm(self.tokens_lists):\n",
    "            dict = {}\n",
    "            for token in tokens:\n",
    "                dict[token] = (dict[token] if token in dict.keys() else 0) + 1\n",
    "            self.frequencies.append(dict) \n",
    "\n",
    "    def get_weights(self):\n",
    "        max = [np.max(list(d.values())) for d in self.frequencies]\n",
    "        self.weights = []\n",
    "        for i in tqdm(range(len(max))):\n",
    "            d = {}\n",
    "            for k, v in self.frequencies[i].items():\n",
    "                d[k] = round((v/max[i]) * np.log10(len(max)/len(self.get_docs(k, preprocessed=True))+1), 2)\n",
    "            self.weights.append(d)\n",
    "        \n",
    "    def combine(self, origin):\n",
    "        out = {}\n",
    "        sets = set()\n",
    "        for o in origin:\n",
    "            sets = sets | set(o)\n",
    "        frequencies = [{k: origin[i].get(k) if origin[i].get(k) else  0 for k in sets} for i in range(len(origin))]\n",
    "        for freq, d in tqdm(zip(frequencies, range(len(frequencies)))):\n",
    "            for k, v in freq.items():\n",
    "                out[(k, d)] = v\n",
    "        return out\n",
    "\n",
    "    def process(self):\n",
    "        print('Tokenizing...')\n",
    "        self.tokenize()\n",
    "        print('Getting frequencies...')\n",
    "        self.get_freq()\n",
    "        print('Combining...')\n",
    "        self.all_frequencies = self.combine(self.frequencies) \n",
    "        self.get_inverted_f()\n",
    "        print('Getting weights...')\n",
    "        self.get_weights()\n",
    "        print('Combining...')\n",
    "        self.all_weights = self.combine(self.weights) \n",
    "        self.get_index()\n",
    "        self.get_inverted()\n",
    "        \n",
    "    def get_index(self):\n",
    "        index = {}\n",
    "        for doc, (w, f) in enumerate(zip(self.weights, self.frequencies)):\n",
    "            d = []\n",
    "            for token in list(w.keys()):\n",
    "                d.append([token, f[token], w[token]])\n",
    "            index[doc] = d\n",
    "        self.index = index\n",
    "\n",
    "    def get_inverted_f(self):\n",
    "        inverted = {}\n",
    "        for doc, f in enumerate(self.frequencies):\n",
    "            for token in list(f.keys()):\n",
    "                if token in inverted.keys():\n",
    "                    inverted[token].append([doc, f[token]]) \n",
    "                else:\n",
    "                    inverted[token] = [[doc, f[token]]]\n",
    "        self.inverted = inverted\n",
    "\n",
    "    def get_inverted(self):\n",
    "        inverted = {}\n",
    "        for doc, (w, f) in enumerate(zip(self.weights, self.frequencies)):\n",
    "            for token in list(w.keys()):\n",
    "                if token in inverted.keys():\n",
    "                    inverted[token].append([doc, f[token], w[token]]) \n",
    "                else:\n",
    "                    inverted[token] = [[doc, f[token], w[token]]]\n",
    "        self.inverted = inverted\n",
    "        \n",
    "    def get_docs_per_token(self, token):\n",
    "        f = {d: v for (t, d), v in self.all_frequencies.items() if token == t} \n",
    "        f = [i for i in list(f.values()) if i != 0]\n",
    "        return f \n",
    "    \n",
    "    def get_docs(self, token, preprocessed=False):\n",
    "        if not preprocessed:\n",
    "            token = self.filter(token.lower())\n",
    "        return self.inverted[token]\n",
    "\n",
    "    def get_docs_query(self, query):\n",
    "        tokens = [token for token in query.split()]\n",
    "        all = {}\n",
    "        details = {}\n",
    "        for token in tokens:\n",
    "            docs = self.get_docs(token)\n",
    "            for d in docs:\n",
    "                if d[0] not in all.keys():\n",
    "                    all[d[0]] = [[d[1]], [d[2]]]\n",
    "                else:\n",
    "                    all[d[0]] = [[all[d[0]][0][0] + d[1]], [round(all[d[0]][1][0] + d[2], 2)]]\n",
    "            details[token] = {d[0]: [d[1], d[2]] for d in docs}\n",
    "        return details, all\n",
    "\n",
    "    def scalar_prod(self, n_doc, query):\n",
    "        result = 0\n",
    "        for token in query:\n",
    "            try:\n",
    "                result += [l[2] for l in self.get_docs(token, preprocessed=True) if l[0] == n_doc][0]\n",
    "            except:\n",
    "                pass\n",
    "        return result\n",
    "\n",
    "    def cosine_measure(self, doc, query):\n",
    "        w = [self.get_docs(token, preprocessed=True)[0][2] for token in doc]\n",
    "        result = np.sqrt(len(query)) * np.sqrt(np.dot(w, w))\n",
    "        return self.scalar_prod(doc, query) / result \n",
    "\n",
    "    def jaccard_measure(self, doc, query):\n",
    "        w = [self.get_docs(token, preprocessed=True)[0][2] for token in doc]\n",
    "        result = len(query) + np.dot(w, w) - self.scalar_prod(doc, query)\n",
    "        return self.scalar_prod(doc, query) / result\n",
    "\n",
    "    def vector_search(self, max_docs=50, metric='scalar'):\n",
    "        queries = np.unique(self.ground_truth['Query'])\n",
    "        relevent_docs = [list(self.ground_truth[self.ground_truth['Query'] == q]['Relevent document']) for q in queries]\n",
    "        predicted = {} \n",
    "        if metric == 'scalar':\n",
    "            metric = self.scalar_prod\n",
    "        elif metric == 'cosine':\n",
    "            metric = self.cosine_measure\n",
    "        elif metric == 'jaccard':\n",
    "            metric = self.jaccard_measure\n",
    "        for q in tqdm(queries):\n",
    "            pred = []\n",
    "            query = self.queries[str(q-1)] \n",
    "            for doc, tokens in self.index.items():\n",
    "                try:\n",
    "                    pred.append([q, doc, metric(doc, query)]) \n",
    "                except:\n",
    "                    pred.append([q, doc, metric([t[0] for t in tokens], query)])\n",
    "            pred = sorted(pred, key=lambda x: x[2], reverse=True)\n",
    "            predicted[q] = [p[1] for p in pred[:max_docs]] \n",
    "        return predicted.values(), relevent_docs\n",
    "\n",
    "    def PR(self, pred, relevent):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        len_TP = []\n",
    "        len_pred = []\n",
    "        for p, r in zip(pred, relevent):\n",
    "            TP = len(set(p) & set(r))\n",
    "            precisions.append(TP/len(p))\n",
    "            recalls.append(TP/len(r)) \n",
    "            len_TP.append(TP)\n",
    "            len_pred.append(len(p))\n",
    "        return np.mean(len_TP), np.mean(len_pred), np.mean(precisions), np.mean(recalls)\n",
    "\n",
    "    def accuracy(self, pred, relevent):\n",
    "        acc = []\n",
    "        for p, r in zip(pred, relevent):\n",
    "            TP = len(set(p) & set(r))\n",
    "            acc.append(TP/len(r))\n",
    "        return np.mean(acc)\n",
    "\n",
    "    def BM25_per_doc(self, n_doc, doc, query, avgdl, N, b=0.75, k1=1.2):\n",
    "        score = 0\n",
    "        for token in query:\n",
    "            try:\n",
    "                temp = self.get_docs(token, preprocessed=True)\n",
    "            except:\n",
    "                temp = None\n",
    "            try:\n",
    "                tf = [l[2] for l in temp if l[0] == n_doc][0]\n",
    "            except:\n",
    "                tf = 0\n",
    "            dl = len(doc)\n",
    "            n = 0\n",
    "            try:\n",
    "                n = len(temp)\n",
    "            except:\n",
    "                pass\n",
    "            idf = math.log((N - n + 0.5) / (n + 0.5))\n",
    "            w = idf * (tf * (k1 + 1) / (tf + k1 * (1 - b + b * dl / avgdl)))\n",
    "            score += w\n",
    "        return score\n",
    "\n",
    "    def BM25(self):\n",
    "        pred = []\n",
    "        queries = np.unique(self.ground_truth['Query'])\n",
    "        N = len(self.index)\n",
    "        avgdl = sum([len(t) for _, t in self.index.items()]) / N\n",
    "        for q in tqdm(queries):\n",
    "            query = self.queries[str(q-1)] \n",
    "            scores = []\n",
    "            for doc, tokens in self.index.items():\n",
    "                scores.append(self.BM25_per_doc(doc, tokens, query, avgdl, N))\n",
    "            pred.append(sorted(zip(range(N), scores), key=lambda x: x[1], reverse=True))\n",
    "        return pred\n",
    "\n",
    "    def new_bm25(self):\n",
    "        bm25 = BM25Okapi([[i[0] for i in j] for j in list(self.index.values())])\n",
    "        queries = np.unique(self.ground_truth['Query'])\n",
    "        pred = []\n",
    "        for q in tqdm(queries):\n",
    "            query = self.queries[str(q-1)] \n",
    "            pred.append(sorted(enumerate(bm25.get_scores(query)), key=lambda x: x[1], reverse=True)) \n",
    "        return pred\n",
    "\n",
    "    '''def bool_search(self, q):\n",
    "        keywords = ['and', 'or', 'not']\n",
    "    \n",
    "        q = q.replace('-', 'not ')\n",
    "        print() \n",
    "        print('q', q) \n",
    "\n",
    "        parsed = [self.filter(token.lower()) for token in q.split()]\n",
    "        parsed = [i for i in parsed if i not in keywords]\n",
    "        print('parsed', parsed) \n",
    "        parsed = [index.filter(i.lower()) for i in parsed]\n",
    "\n",
    "        _, freq, _ = self.get_freq_tokens(' '.join(parsed))\n",
    "        freq = {k: {d: 1 if f else 0 for d, f in v.items()} for k, v in freq.items()}\n",
    "\n",
    "        print('freq', freq)    \n",
    "        q = q.lower()\n",
    "        r = [q, q, q, q] \n",
    "\n",
    "        for k, v in freq.items():\n",
    "            for d, f in v.items():\n",
    "                r[d] = r[d].replace(k, str(f))\n",
    "\n",
    "        print('r', r)'''\n",
    "\n",
    "    def bool_search(self, q):\n",
    "        keywords = ['and', 'or', 'not']\n",
    "        \n",
    "        q = q.replace('-', 'not ')\n",
    "        q = ' '.join([self.filter(token.lower()) for token in q.split()])\n",
    "    \n",
    "        print()\n",
    "        print('q', q)\n",
    "\n",
    "        parsed = [self.filter(token.lower()) for token in q]\n",
    "        parsed = [i for i in parsed if i not in keywords]\n",
    "        print('parsed', parsed)\n",
    "        parsed = [index.filter(i.lower()) for i in parsed]\n",
    "\n",
    "        _, freq, _ = self.get_freq_tokens(' '.join(parsed))\n",
    "        freq = {k: {d: 1 if f else 0 for d, f in v.items()} for k, v in freq.items()}\n",
    "\n",
    "        print('freq', freq)    \n",
    "\n",
    "        r = [q, q, q, q] \n",
    "\n",
    "        for k, v in freq.items():\n",
    "            for d, f in v.items():\n",
    "                r[d] = r[d].replace(k, str(f))\n",
    "        print('r', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('queries.json') as f:\n",
    "    queries = json.load(f) \n",
    "ground_truth = pd.read_csv('ground_truth.csv', sep=',')\n",
    "index = Index((index.index, index.inverted, queries, ground_truth), preprocessed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:46<00:00,  1.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0789473684210527, 20.0, 0.053947368421052626, 0.030589365932939688)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, relevent_docs = index.vector_search(max_docs=20, metric='scalar') \n",
    "index.PR(list(predicted), relevent_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [01:00<00:00,  1.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.144736842105264, 200.0, 0.04072368421052632, 0.2133032768890171)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, relevent_docs = index.vector_search(max_docs=200, metric='cosine')\n",
    "index.PR(list(predicted), relevent_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [01:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.144736842105264, 200.0, 0.04072368421052632, 0.2133032768890171)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, relevent_docs = index.vector_search(max_docs=200, metric='jaccard') \n",
    "index.PR(list(predicted), relevent_docs) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [01:02<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.276315789473684, 50.0, 0.04552631578947369, 0.06604444158123768)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatGPT's\n",
    "p = index.BM25() \n",
    "index.PR([[i[0] for i in j[:50]] for j in p], relevent_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:00<00:00, 92.61it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.6842105263157894, 50.0, 0.05368421052631579, 0.07072597750776016)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dude on Kaggle's\n",
    "p = index.new_bm25()\n",
    "index.PR([[i[0] for i in j[:50]] for j in p], relevent_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5addf786bcd861d1ce5006f23111f8cbb206731e5b61b0a5632ba9e0252558a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
